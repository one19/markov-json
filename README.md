# MARKOV-JSON

[![Maintenance status](https://raw.githubusercontent.com/one19/project-status/master/cache/markov-json/maintained.svg?sanitize=true)](https://github.com/one19/project-status) [![published on npm!](https://raw.githubusercontent.com/one19/project-status/master/cache/markov-json/npm.svg?sanitize=true)](https://www.npmjs.com/package/markov-json) [![Very unstable code](https://raw.githubusercontent.com/one19/project-status/master/cache/markov-json/maintenance.svg?sanitize=true)](https://github.com/one19/project-status)

---

### What is this?

A markov generator of n=2 complexity. It's real real simple.

You instantiate it:

```
import * as Markov from 'markov-json';
const mkj = new Markov();
```

or, if you've already got a file /made with this library/ to parse:
`const mkj = new Markov('./thatcrazymarkov.json');`
or if you've created one inline somehow, it'll be supported to import there also, soon.

**Then the cool stuff starts!**

### TRAINING:

Markov generators are only really good if they're trained. Training this one is **super** simple!\
It's just a function!

Pass it text that vaguely looks like a language, and this package does the rest. It doesn't matter if you're passing it paragraphs, or sentences, tweets, or novels, all you have to do is:

```
mkv.train('some cool words');
```

### WHAT THEN!?

Then, all that's left is to get it to spit words back at you! It'll vaguelly look like whatever you trained it on. Give it Shakespeare, and it'll shake a spear at you back.

```
mkv.blob(NUMBER_OF_WORDS_ID_LIKE)
```

Alternatively, you could ask it for sentences. Better hope whatever you're parsing used some line-end punctuation, or it'll cycle **FOREVER**

```
mkv.sentences(NUMBER_OF_SENTENCES)
```

### AFTERWARDS:

Finally, if you'd like to store your training model someplace, it's as simple as `mkv.output('./filepath_someplace.json')`, or alternatively, you could instead get the state object by not passing it any arguments _(like, say, if you wanted to send it someplace on the internets)_ `mkv.output()`.

## BEHOLD!

The first sentences generated by my package after training on hamlet _with_ **absolutely** _no massaging done to the text taken from [mit](http://shakespeare.mit.edu/hamlet/full.html)_

> ma.sentence();
> `'What, a dew!'`
> ma.sentence();
> `'Must be toward france; the skirts of heaven visit her galled eyes, a little month: such impress of those foresaid lands lost, cousin, as common.'`
> ma.sentence();
> `'A king claudius we doubt it was sick almost to see you.'`
> ma.sentence(5);
> `'So hallow\'d and i pray thee do mine ear that lives must hold my tongue. Hamlet not for thy asking? Marcellus. Horatio a man might be and the extravagant and bed-rid, for so. This portentous figure like a guilty thing to france and thy nighted colour off, colleagued with remembrance of our duty.'`

### TODO:

1.  instead of /just/ passing it a JSON file, I'd like it to parse a text and train on it
2.  precommit linting/compilation is borked, it should be fixed
3.  Add **super-neato** 100k iteration tests to see if the character distribution stays consistent to the trained info
4.  Extend it to work with nx complexity _(at obviously increased time cost)_
